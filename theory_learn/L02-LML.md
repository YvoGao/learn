# 终身学习

> 终身学习是机器学习和神经网络系统长期面临的挑战。在现实世界中运行的计算系统暴露于连续的信息流中，因此需要从动态数据分布中学习和记忆多个任务。随着时间的推移，通过适应新知识而不断学习，同时保留以前学习的经验的能力被称为持续终身学习。

**关键问题：**灾难性遗忘

早期缓解灾难性遗忘的尝试通常由存储以前数据的记忆系统组成，并定期重放旧样本和从新数据中提取的样本，这些方法至今仍在使用

给予内存空间的解决方法：

（i）为新知识分配额外的神经资源；

（ii）如果资源固定，则使用非重叠表示；

（iii）在表示新信息时交错旧知识。

## 生物学和灾难性遗忘

### 正则化方法

- a）在正则化的同时进行再培训，以防止以前学习过的任务发生灾难性遗忘；
- b）使用网络扩展表示新任务的参数不变；
- c）使用可能扩展的选择性再培训。

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20211122124320574.png" alt="image-20211122124320574" style="zoom:70%;" />

总之，正则化方法提供了一种在一定条件下缓解灾难性遗忘的方法。然而，它们包含额外的损失条款，用于保护整合的知识，在神经资源有限的情况下，可能会导致新旧任务性能的权衡。



### 动态神经元体系结构

- 2016年Rusu et al提出固定子网络，随着任务增加网络
- Zhou等人（2012）提出了去噪自动编码器的增量训练，该编码器为高损耗样本添加神经元，然后将这些神经元与现有神经元合并，以防止冗余，在这种情况下，只有最顶层可以生长，普通的反向传播训练过程效率低下。
- Y oon et al.（2018）将这一概念引入监督学习范式，并提出了一种动态扩展网络（DEN），该网络增加了可训练参数的数量，以增量学习新任务。
- Part&Lemon（20162017）提出将预先训练好的CNN与自组织增量神经网络（SOINN）相结合，以利用CNN良好的表示能力，同时允许分类网络根据连续对象识别场景中的任务需求增长

### 互补学习系统与记忆回放

- 双记忆学习系统在不同程度上受到了CLS理论的启发，以解决灾难性遗忘问题。Hinton&Plaut（1987）提出了这一概念的早期计算示例，其中每个突触连接都有两个权重：一个具有缓慢变化率的塑料权重，用于存储长期知识，另一个快速变化的权重用于临时知识。
- Gepperth&Karaoguz（2015）提出了两种增量学习方法：（i）改进的自组织映射（SOM）和（ii）使用短期记忆（STM）扩展的SOM。
- Shin等人（2017年）受海马体在再现先前编码经验中的生成作用的启发，提出了一种由深层生成模型和任务求解器组成的双模型架构。
- Lüders et  al.（2016）提出了一种新的图灵机器（ENTM），通过逐步分配额外的外部内存组件，使代理能够存储长期内存。通过不断演化网络拓扑和权值，从初始最小配置中找到持续学习网络的最优结构。
- Lopez Paz&Ranzato（2017）提出了梯度情景记忆（GEM）模型，该模型能够将知识正向转移到先前的任务中。
- Kemker&Kanan（2018年）提出了渐进式课堂学习的FearNet模型，该模型的灵感来源于恐惧条件作用期间哺乳动物大脑中的回忆和巩固研究。
- Parisi、Tani、Weber和Wermter（2018）提出了一种双记忆自组织体系结构，用于终身学习视频中的时空表示。

### 评价指标

- Kemker等人（2018年）提出了一套评估终身学习方法的指南，并进行了补充实验，对多种方法进行了直接定量比较。这些指导原则包括使用三个基准实验：（i）数据置换，（ii）增量课堂学习和（iii）多模式学习。数据置换实验包括使用数据集以及同一数据集的置换版本来训练模型，测试模型以类似特征表示递增学习新信息的能力。然后期望该模型在随后的随机排列数据样本学习过程中防止原始数据的灾难性遗忘。在增量课堂学习实验中，模型的性能反映了它在一次增量学习一个班级的同时保留先前学习信息的能力。最后，在多模式学习实验中，使用不同模式的数据集对同一模型进行顺序训练，以测试该模型增量学习具有显著不同特征表示的新信息的能力（例如，首先学习图像分类数据集，然后学习音频分类数据集）。
- Lomonaco&Maltoni（2017）提出了CORe50，这是一种用于连续对象识别的新型数据集，包括从不同角度观察到的50类对象，包括背景、照明、模糊、遮挡、姿势和比例的变化。除了数据集之外，作者还提出了三种增量学习场景：（i）新实例（NI），其中所有类在第一批中显示，而已知类的后续实例随着时间的推移变得可用，新实例（NC），其中，对于每个连续批，新的对象类可用，因此模型必须处理新类的学习，而不会忘记以前学习过的类。



## 发展途径和自主主体

**通过内在动机的探索自主学习新任务和技能的能力是区分生物终身学习与当前连续神经网络分类模型的主要因素之一。**

### 走向自主

通过内在动机的探索自主学习新任务和技能的能力是区分生物终身学习与当前连续神经网络分类模型的主要因素之一。



### 发展性学习和课程学习

发展性学习策略已经在嵌入式智能体上进行了实验，以实时调节与环境的具体互动（Cangelosi&Schlesinger2015，Tani  2016）。与以成批信息为输入的计算模型不同，发展代理基于其自主的感觉运动体验获得越来越复杂的技能。因此，阶段性开发对于以较少的辅导经验提升认知技能至关重要，在人工智能中是一个非常复制的过程。

### 迁移学习

通过低层次特征和高层次特征之间不同层转换，达到多任务学习的目的。

### 好奇心与内在动机

内在动机的计算模型可以通过在线（自我）生成学习课程（Baranes&Oudeyer2013、Forestier&Oudeyer2016）来收集数据并逐步获得技能。

### 多传感学习

自不同传感器模式（例如视觉、音频、本体感觉）的信息可集成到多传感器表示中，或用于增强单传感器表示。从计算的角度来看，对多传感器学习建模有很多好处。首先，多传感器函数的目标是在感觉输入不确定和不明确的情况下产生鲁棒响应。因果推理模型已应用于包括暴露于不一致视听信息的场景，以解决多感官冲突（Parisi、Barros、Kerzel、Wu、Yang、Li、Liu和Wermter2017、Parisi、Barros、Fu、Magg.，Wu、Liu和Wermter  2018）。c

