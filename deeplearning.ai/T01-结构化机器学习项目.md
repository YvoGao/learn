## 正交化

- 成本函数不能很好地拟合训练集——更大网络，更好地优化算法
- 拟合性很差（训练集可以，但是验证集不行）——增大训练集
- 验证集可以，测试集不行——使用更大的验证集
- 在真实数据中表现的不行——调整成本函数和和网络

## 单一数字评估

- 将多个评价指标合成一个实数——方便选取好的模型
- 设立满足需求的指标
- 不方便整合时：只用一种指标，但是其他指标设置阈值

## 设立数据集

- 验证集和训练集相同分布——目标一致
- 训练集和验证集和训练集的分布比例——6：2：2，数据过大改变

- 当评价指标更小，但是不满足用户需求的，要改变，可能会是适当加上权重



## 为什么像人的表现

- 超过人之间进步很快
- 超过人后进展缓慢——都达不到贝叶斯最优解



## 可避免偏差

- 人类表现决定了模型的最好效果——接近贝叶斯误差

> 如果训练集和人类水平差的多，可以增大模型，多训练几次提高
>
> 如果差的不多，那很容易就过拟合

训练姐效果与人类误差的差值称为可避免偏差，训练集与测试集的差值叫方差。

## 误差分析

- 查看错误原因，对考研大幅减少误差的方向进行优化
- 在表格中备注失败原因
- 清楚是否有标注错误的数据

## 快速搭建一个系统

- 设置数据集
- 建立模型（能用的就行）
- 分析优化模型

## 数据不匹配

- 人工去看验证集
- 合成数据集（容易过拟合）

## 迁移学习

- 图像检测：把最后一层拿掉，随机赋予权重训练
- 语音识别：去掉最后一层，增加多层

> 适用于：迁移目标没有太多的样本，利用之前的任务中大量数据学到的东西
>
> - 任务A和B有相同的输入
> - 任务A的训练集数量更广，任务B数据集少
> - 任务A的低层网络对任务B有用

## 多任务学习

- 训练一个足够大的网络完成多个任务，但是其他任务的数据集要大于任务A的数据集

> 要求：
>
> - 训练一系列任务在底层可以公用
> - 每一个任务的数据量是相似的
> - 能训练一个足够大的网络

## 端到端的神经网络

直接学习x到y的映射，需要大量数据，数据较小时，传统的较有用

> 优点:
>
> - 少手工设计
> - 处理数据大
>
> 缺点：
>
> - 需要更多的数据
> - 缺少了手工设计对数据的处理

